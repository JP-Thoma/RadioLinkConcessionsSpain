{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5975c92",
   "metadata": {},
   "source": [
    "# Cleaning the datasets for Registro Público de Concesiones and zipcodes\n",
    "\n",
    "This code includes all the cleaning done for the datasets that were scrapped and aggregates these into a big dataset.\n",
    "\n",
    "- [Zipcodes](https://xn--cdigospostales-lob.es/listado-de-codigos-postales-de-espana/)\n",
    "- [Main dataset](https://sedeaplicaciones.minetur.gob.es/RPC_Consulta)\n",
    "    - Main page dataset\n",
    "    - Pop-up datasets\n",
    "\n",
    "## Packages used\n",
    "\n",
    "- datetime\n",
    "- numpy\n",
    "- pandas\n",
    "- fuzzywuzzy\n",
    "- math\n",
    "- datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b7d4ed",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a7b1305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from datetime import datetime as dt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99dacd",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "07242ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset\n",
    "main_df = pd.read_csv('/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/RegistroPublicoConcesiones_General.csv')\n",
    "# Zipcodes dataset\n",
    "zipcodes = pd.read_excel('/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/new_data/listado-codigos-postales-con-LatyLon.xls')\n",
    "# Pop-up\n",
    "df = pd.read_csv('/Users/niko/Downloads/RegistroPublicoConcesiones_Frequencies.csv',sep=';')\n",
    "\n",
    "# Import all the datasets for all pop-ups\n",
    "# region_1 = pd.read_csv('')\n",
    "# region_2 = pd.read_csv('')\n",
    "# region_3 = pd.read_csv('')\n",
    "# region_4 = pd.read_csv('')\n",
    "# region_5 = pd.read_csv('')\n",
    "# region_6 = pd.read_csv('')\n",
    "# region_7 = pd.read_csv('')\n",
    "# region_8 = pd.read_csv('')\n",
    "# region_9 = pd.read_csv('')\n",
    "# region_10 = pd.read_csv('')\n",
    "# region_11 = pd.read_csv('')\n",
    "# region_12 = pd.read_csv('')\n",
    "# region_13 = pd.read_csv('')\n",
    "# region_14 = pd.read_csv('')\n",
    "# region_15 = pd.read_csv('')\n",
    "# region_16 = pd.read_csv('')\n",
    "# region_17 = pd.read_csv('')\n",
    "# region_18 = pd.read_csv('')\n",
    "# region_19 = pd.read_csv('')\n",
    "\n",
    "# Concat all the datasets\n",
    "# df = pd.concat([\n",
    "#                         region_1,\n",
    "#                         region_2,\n",
    "#                         region_3,\n",
    "#                         region_4,\n",
    "#                         region_5,\n",
    "#                         region_6,\n",
    "#                         region_7,\n",
    "#                         region_8,\n",
    "#                         region_9,\n",
    "#                         region_10,\n",
    "#                         region_11,\n",
    "#                         region_12,\n",
    "#                         region_13,\n",
    "#                         region_14,\n",
    "#                         region_15,\n",
    "#                         region_16,\n",
    "#                         region_17,\n",
    "#                         region_18,\n",
    "#                         region_19,\n",
    "#                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ac1b8d",
   "metadata": {},
   "source": [
    "# Clean the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e38120",
   "metadata": {},
   "source": [
    "## Clean the pop-up data\n",
    "Doing a fuzzy join on the dataset of zipcodes and the data of reference <br>\n",
    "data inside the 'Consulta del Registro Público de Concesiones'.\n",
    "This is needed, because the data from the source does not have zipcodes, longitute and latitude.<br>\n",
    "The fuzzy join does a cross join, then calculates the fuzzy ratio and include the data based on a fuzzy_ratio.<br>\n",
    "Then, the duplicated columns are dropped.<br>\n",
    "The output is a new dataset that includes the zipcode, longitude and latitude per tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "67f0053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c7299195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all the pop-up datasets\n",
    "# df = df.drop(df.columns[[0]],axis = 1)\n",
    "# df = df.drop(df.columns[[0]],axis = 1)\n",
    "df = df.apply(lambda x: x.str.strip()).replace('', np.nan)\n",
    "df = df.fillna(method='ffill')\n",
    "df[['Frequencias', 'Tipo']] = df['Frecuencia'].str.split(' ', 1, expand=True)\n",
    "del df['Frecuencia']\n",
    "df['Frequencias'] = df['Frequencias'].apply(lambda x: x.replace('.', '')).apply(lambda x: x.replace(',', '.')).astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9e2120",
   "metadata": {},
   "source": [
    "### Cleaning of zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "87af28b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipcodes.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d052bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns that we want to include in the final dataset\n",
    "zipcodes_columns = ['codigopostalid','lat','lon']\n",
    "df_columns = [ 'Referencia','Comunidad','Provincia','Municipio','Frequencias','Tipo']\n",
    "all_new_columns = df_columns+zipcodes_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067fd86d",
   "metadata": {},
   "source": [
    "### Join pop-ups and zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1740197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fuzzy ratio used to include the data\n",
    "fuzzy_ratio = 20\n",
    "\n",
    "# Create new column to use to join both datasets\n",
    "zipcodes['merge']='all'\n",
    "df['merge']='all'\n",
    "\n",
    "# Join both datasets per row\n",
    "all_datasets = pd.merge(df,zipcodes,on='merge')\n",
    "del all_datasets['merge']\n",
    "\n",
    "# Create list of tuples based on the columns that we want to use for the join\n",
    "datasets_tuple = all_datasets[['Municipio', 'poblacion']].apply(tuple, axis=1).tolist()\n",
    "\n",
    "# Create the fuzz ratio on the list of tuples ceated\n",
    "all_datasets['ratio'] = [fuzz.token_sort_ratio(*i) for i in datasets_tuple]\n",
    "\n",
    "# Exclude those that have a low match ratio, the threshhold is set low because some matches have a low score\n",
    "all_datasets = all_datasets[all_datasets.ratio>fuzzy_ratio]\n",
    "\n",
    "# Drop all duplicates based on the defined columns and keep all the wanted ones\n",
    "final_df = all_datasets[all_new_columns].drop_duplicates(subset=['Referencia','Municipio','Frequencias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0673e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate step to save the data\n",
    "final_df.to_csv('/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/popup_zipcodes.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a518eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "12b667d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b2546",
   "metadata": {},
   "source": [
    "## Clean the main dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "08466cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows that are unecessary\n",
    "# Function to return a list with unique numeric values\n",
    "def unique(list1):\n",
    "    x = np.array(list1)\n",
    "    return list(np.unique(x))\n",
    "\n",
    "# Creates list of the returned values \n",
    "list_cities = unique(main_df.Localidad)\n",
    "\n",
    "# Includes only text values, all cities of Spain\n",
    "new_list_cities = []\n",
    "for i in list_cities:\n",
    "    if i.isnumeric() is False:\n",
    "        if i != ' ':\n",
    "            new_list_cities.append(i)\n",
    "\n",
    "# Filter out all the values that are not inside the new_list_cities\n",
    "main_df = main_df.loc[main_df['Localidad'].isin(new_list_cities)]\n",
    "\n",
    "# --- Work on booleans\n",
    "\n",
    "# Fill in False to all nulls for specific boolean columns\n",
    "main_df[['Susceptible cesion','Susceptible mutualizacion','Obtenido por transferencia']] = \\\n",
    "                            main_df[['Susceptible cesion','Susceptible mutualizacion','Obtenido por transferencia']].fillna(False)\n",
    "                            \n",
    "main_df['Susceptible cesion'] = main_df['Susceptible cesion'].replace(\"true\", True)\n",
    "main_df['Obtenido por transferencia'] = main_df['Obtenido por transferencia'].replace(\"Detalle\", True)\n",
    "\n",
    "# --- Work on the dates\n",
    "\n",
    "# Select columns that contain dates\n",
    "date_columns = ['F. Caducidad','F. Concesion']\n",
    "\n",
    "# Transform date objects to datetime\n",
    "main_df[date_columns] = main_df[date_columns].apply(pd.to_datetime, errors='coerce',infer_datetime_format=True)\n",
    "\n",
    "# New features day, month and year\n",
    "main_df['dia_concesion'] = round((main_df['F. Caducidad'] - main_df['F. Concesion']).dt.days,0).fillna(0).apply(np.int64)\n",
    "main_df['mes_concesion'] = round((main_df['F. Caducidad'] - main_df['F. Concesion']).dt.days/12).fillna(0).apply(np.int64)\n",
    "main_df['año_concesion'] = round((main_df['F. Caducidad'] - main_df['F. Concesion']).dt.days/360).fillna(0).apply(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb71313",
   "metadata": {},
   "source": [
    "## Joined datasets\n",
    "Joining both datasets to create the final dataset that needs to be standarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "262c2329",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = pd.merge(main_df,final_df, how='left', left_on = 'Referencia', right_on = 'Referencia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0bb6006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate step to clean the data\n",
    "df_joined.to_csv('/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/dj_joined.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f04e6f",
   "metadata": {},
   "source": [
    "### Total shape of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e36fb089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pop-up and zipcodes.\n",
      " Columns: 9 & Rows: 3133\n",
      "\n",
      "Main dataset.\n",
      " Columns: 15 & Rows: 5209\n",
      "\n",
      "Final dataset with zipcodes, pop-ups and the main page.\n",
      " Columns: 23 & Rows: 7554\n",
      "\n",
      "Total % loss of rows of the final dataset: 9.45\n",
      "Total absolute loss of rows of the final dataset: 788\n"
     ]
    }
   ],
   "source": [
    "print(f'Pop-up and zipcodes.\\n Columns: {final_df.shape[1]} & Rows: {final_df.shape[0]}\\n')\n",
    "print(f'Main dataset.\\n Columns: {main_df.shape[1]} & Rows: {main_df.shape[0]}\\n')\n",
    "print(f'Final dataset with zipcodes, pop-ups and the main page.\\n Columns: {df_joined.shape[1]} & Rows: {df_joined.shape[0]}\\n')\n",
    "\n",
    "total_rows = final_df.shape[0]+main_df.shape[0]\n",
    "final_total_rows = df_joined.shape[0]\n",
    "absolute_values = total_rows-final_total_rows\n",
    "percentage_final_rows = round(1-final_total_rows/total_rows,4)*100\n",
    "\n",
    "\n",
    "print(f'Total % loss of rows of the final dataset: {percentage_final_rows}\\nTotal absolute loss of rows of the final dataset: {absolute_values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a549c",
   "metadata": {},
   "source": [
    "# Standarization of both datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770fae7b",
   "metadata": {},
   "source": [
    "## Clean of 'Titular'\n",
    "Try to remove as many duplicated entries and wrongly entered data and identify unique vendors.<br>\n",
    "We have entries that include for example 'Vodafone Ono' and 'Vodafone Espagna'. While these companies are not the same, they are from the same mother company.<br>\n",
    "We needed to re-join the dataset and due to the fuzzy join we did to clean this data, we lost some rows. This cumulates to approximately 13.68% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "57e6f98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7554 entries, 0 to 7553\n",
      "Data columns (total 23 columns):\n",
      " #   Column                      Non-Null Count  Dtype         \n",
      "---  ------                      --------------  -----         \n",
      " 0   Referencia                  7554 non-null   object        \n",
      " 1   Titular                     7554 non-null   object        \n",
      " 2   NIF/CIF                     7554 non-null   object        \n",
      " 3   Domicilio social            7554 non-null   object        \n",
      " 4   Localidad                   7554 non-null   object        \n",
      " 5   Provincia_x                 7554 non-null   object        \n",
      " 6   C. Postal                   7554 non-null   int64         \n",
      " 7   F. Concesion                2966 non-null   datetime64[ns]\n",
      " 8   F. Caducidad                7553 non-null   datetime64[ns]\n",
      " 9   Susceptible cesion          7554 non-null   bool          \n",
      " 10  Susceptible mutualizacion   7554 non-null   bool          \n",
      " 11  Obtenido por transferencia  7554 non-null   bool          \n",
      " 12  dia_concesion               7554 non-null   int64         \n",
      " 13  mes_concesion               7554 non-null   int64         \n",
      " 14  año_concesion               7554 non-null   int64         \n",
      " 15  Comunidad                   3234 non-null   object        \n",
      " 16  Provincia_y                 3234 non-null   object        \n",
      " 17  Municipio                   3234 non-null   object        \n",
      " 18  Frequencias                 3234 non-null   float64       \n",
      " 19  Tipo                        3234 non-null   object        \n",
      " 20  codigopostalid              3234 non-null   float64       \n",
      " 21  lat                         3234 non-null   float64       \n",
      " 22  lon                         3234 non-null   float64       \n",
      "dtypes: bool(3), datetime64[ns](2), float64(4), int64(4), object(10)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_joined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "49553331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if nulls in dataset\n",
    "# [print(i) for i in df_joined['Titular'].isna() if i == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "46c5ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in df_joined[['Titular']]:\n",
    "#     df_joined[col] = df_joined[col].str.strip()\n",
    "#     print('Number of unique values in ' + str(col) +': ' + str(df[col].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d2b5551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique owners\n",
    "unique_titular = df_joined['Titular'].unique().tolist()\n",
    "\n",
    "#Create tuples of brand names, matched brand names, and the score\n",
    "score_sort = [(x,) + i\n",
    "             for x in unique_titular \n",
    "             for i in process.extract(x, unique_titular,     scorer=fuzz.token_sort_ratio)]\n",
    "#Create a dataframe from the tuples\n",
    "similarity_sort = pd.DataFrame(score_sort, columns=['brand_sort','match_sort','score_sort'])\n",
    "\n",
    "similarity_sort['sorted_brand_sort'] = np.minimum(similarity_sort['brand_sort'], similarity_sort['match_sort'])\n",
    "\n",
    "\n",
    "high_score_sort =  \\\n",
    "similarity_sort[(similarity_sort['score_sort'] >= 80) &\n",
    "                (similarity_sort['brand_sort'] !=  similarity_sort['match_sort']) &\n",
    "                (similarity_sort['sorted_brand_sort'] != similarity_sort['match_sort'])]\n",
    "high_score_sort = high_score_sort.drop('sorted_brand_sort',axis=1).copy()\n",
    "\n",
    "grouped_high_score = high_score_sort.groupby(['brand_sort','score_sort'],as_index=False).agg(\n",
    "                        {'match_sort': ', '.join}).sort_values(\n",
    "                        ['score_sort'], ascending=False)\n",
    "\n",
    "                        # Create new column to use to join both datasets\n",
    "fuzzy_ratio_final = 60\n",
    "\n",
    "grouped_high_score['merge']='all'\n",
    "df_joined['merge']='all'\n",
    "\n",
    "# Join both datasets per row\n",
    "all_datasets = pd.merge(df_joined,grouped_high_score,on='merge')\n",
    "del all_datasets['merge']\n",
    "\n",
    "# Create list of tuples based on the columns that we want to use for the join\n",
    "datasets_tuple = all_datasets[['Titular', 'match_sort']].apply(tuple, axis=1).tolist()\n",
    "\n",
    "# Create the fuzz ratio on the list of tuples ceated\n",
    "all_datasets['ratio'] = [fuzz.token_sort_ratio(*i) for i in datasets_tuple]\n",
    "\n",
    "# Exclude those that have a low match ratio, the threshhold is set low because some matches have a low score\n",
    "all_datasets = all_datasets[all_datasets.ratio>fuzzy_ratio_final]\n",
    "\n",
    "# Drop all duplicates based on the defined columns and keep all the wanted ones\n",
    "df_re_joined = all_datasets.drop_duplicates(subset=['Referencia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5f07d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate step to clean the data\n",
    "df_re_joined.to_csv('/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/final_cleansed_standarized.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "775d8a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First transformation\n",
      " ------------------------------------------------------------\n",
      "Pop-up and zipcodes.\n",
      " Columns: 9 & Rows: 3133\n",
      "\n",
      "Main dataset.\n",
      " Columns: 15 & Rows: 5209\n",
      "\n",
      "Final dataset with zipcodes, pop-ups and the main page.\n",
      " Columns: 24 & Rows: 7554\n",
      "\n",
      "Total % loss of rows of the final dataset: 9.45\n",
      "Total absolute loss of rows of the final dataset: 788\n",
      "\n",
      "Last transformation\n",
      " ------------------------------------------------------------\n",
      "Columns: 24 & Rows: 7554\n",
      "\n",
      "Comparison with original dataset\n",
      "Total loss of originaldata when joining back at datasets with current threshhold (60): 4.9\n",
      "Total absolute loss of rows of the final dataset: 255\n",
      "\n",
      "\n",
      "Comparison with final cleansed dataset\n",
      "Total loss of originaldata when joining back at datasets with current threshhold (60): 34.42\n",
      "Total absolute loss of rows of the final dataset: 2600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('First transformation\\n','--'*30)\n",
    "\n",
    "print(f'Pop-up and zipcodes.\\n Columns: {final_df.shape[1]} & Rows: {final_df.shape[0]}\\n')\n",
    "print(f'Main dataset.\\n Columns: {main_df.shape[1]} & Rows: {main_df.shape[0]}\\n')\n",
    "print(f'Final dataset with zipcodes, pop-ups and the main page.\\n Columns: {df_joined.shape[1]} & Rows: {df_joined.shape[0]}\\n')\n",
    "\n",
    "total_rows = final_df.shape[0]+main_df.shape[0]\n",
    "final_total_rows = df_joined.shape[0]\n",
    "absolute_values = total_rows-final_total_rows\n",
    "percentage_final_rows = round(1-final_total_rows/total_rows,4)*100\n",
    "\n",
    "print(f'Total % loss of rows of the final dataset: {percentage_final_rows}\\nTotal absolute loss of rows of the final dataset: {absolute_values}\\n')\n",
    "\n",
    "print('Last transformation\\n','--'*30)\n",
    "print(f'Columns: {df_joined.shape[1]} & Rows: {df_joined.shape[0]}\\n')\n",
    "\n",
    "absolute_loss_from_original_df = main_df.shape[0] - df_re_joined.shape[0]\n",
    "total_loss_from_original_df = round(1-df_re_joined.shape[0]/main_df.shape[0],4)*100\n",
    "\n",
    "print('Comparison with original dataset')\n",
    "print(f\"Total loss of originaldata when joining back at datasets with current threshhold ({fuzzy_ratio_final}): {total_loss_from_original_df}\\nTotal absolute loss of rows of the final dataset: {absolute_loss_from_original_df}\\n\")\n",
    "absolute_loss_from_cleansed_df = df_joined.shape[0] - df_re_joined.shape[0]\n",
    "total_loss_from_cleansed_df = round(1-df_re_joined.shape[0]/df_joined.shape[0],4)*100\n",
    "print('\\nComparison with final cleansed dataset')\n",
    "print(f'Total loss of originaldata when joining back at datasets with current threshhold ({fuzzy_ratio_final}): {total_loss_from_cleansed_df}\\nTotal absolute loss of rows of the final dataset: {absolute_loss_from_cleansed_df}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7df51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
