{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5975c92",
   "metadata": {},
   "source": [
    "# Cleaning the datasets for Registro Público de Concesiones and zipcodes\n",
    "\n",
    "This code includes all the cleaning done for the datasets that were scrapped and aggregates these into a big dataset.\n",
    "\n",
    "- [Zipcodes](https://xn--cdigospostales-lob.es/listado-de-codigos-postales-de-espana/)\n",
    "- [Main dataset](https://sedeaplicaciones.minetur.gob.es/RPC_Consulta)\n",
    "    - Main page dataset\n",
    "    - Pop-up datasets\n",
    "\n",
    "## Packages used\n",
    "\n",
    "- datetime\n",
    "- numpy\n",
    "- pandas\n",
    "- fuzzywuzzy\n",
    "- math\n",
    "- datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b7d4ed",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b1305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from datetime import datetime as dt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99dacd",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07242ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset\n",
    "main_df = pd.read_csv('/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/RegistroPublicoConcesiones_General.csv')\n",
    "# Zipcodes dataset\n",
    "zipcodes = pd.read_csv('/Users/niko/Downloads/listado-codigos-postales-con-LatyLon.csv',sep=';',dtype={0:str, 1:str, 2:str, 3:object,4:object})\n",
    "# Pop-up\n",
    "df = pd.read_csv('/Users/niko/Downloads/RegistroPublicoConcesiones_Frequencies_Castilla_LaMancha.csv',sep=';')\n",
    "# Population per region\n",
    "population = pd.read_excel('/Users/niko/Downloads/pobmun21.xlsx',dtype={'CODIGO POSTAL':object})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ac1b8d",
   "metadata": {},
   "source": [
    "# Clean the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e38120",
   "metadata": {},
   "source": [
    "## Clean the pop-up data\n",
    "Doing a fuzzy join on the dataset of zipcodes and the data of reference <br>\n",
    "data inside the 'Consulta del Registro Público de Concesiones'.\n",
    "This is needed, because the data from the source does not have zipcodes, longitute and latitude.<br>\n",
    "The fuzzy join does a cross join, then calculates the fuzzy ratio and include the data based on a fuzzy_ratio.<br>\n",
    "Then, the duplicated columns are dropped.<br>\n",
    "The output is a new dataset that includes the zipcode, longitude and latitude per tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76242fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67f0053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(50)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7299195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all the pop-up datasets\n",
    "df = df.apply(lambda x: x.str.strip()).replace('', np.nan)\n",
    "df = df.fillna(method='ffill')\n",
    "df[['Frequencias', 'Tipo']] = df['Frecuencia'].str.split(' ', 1, expand=True)\n",
    "del df['Frecuencia']\n",
    "df['Frequencias'] = df['Frequencias'].apply(lambda x: x.replace('.', '')).apply(lambda x: x.replace(',', '.')).astype('float')\n",
    "df['Frequencias'] = np.where(df['Tipo'] == 'MHz',\n",
    "                                           df['Frequencias'] / 1000,\n",
    "                                           df['Frequencias'])\n",
    "del df['Tipo']\n",
    "df = df.rename(columns={\"Provincia\": \"referencia_Provincia\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9e2120",
   "metadata": {},
   "source": [
    "### Cleaning of zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87af28b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipcodes.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d052bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns that we want to include in the final dataset\n",
    "zipcodes_columns = ['codigopostalid','lat','lon']\n",
    "df_columns = [ 'Referencia','Comunidad','referencia_Provincia','Municipio','Frequencias']\n",
    "all_new_columns = df_columns+zipcodes_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067fd86d",
   "metadata": {},
   "source": [
    "### Join pop-ups and zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1740197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fuzzy ratio used to include the data\n",
    "fuzzy_ratio = 80\n",
    "\n",
    "# Drop duplicates of the zipcodes on the poblacion (Municipio) and stay with the first option\n",
    "zipcodes = zipcodes.drop_duplicates(subset=['poblacion'], keep='first')\n",
    "\n",
    "# Create new column to use to join both datasets\n",
    "zipcodes['merge']='all'\n",
    "df['merge']='all'\n",
    "\n",
    "# Join both datasets per row\n",
    "all_datasets = pd.merge(df,zipcodes,on='merge')\n",
    "\n",
    "# Create list of tuples based on the columns that we want to use for the join\n",
    "datasets_tuple = all_datasets[['Municipio', 'poblacion']].apply(tuple, axis=1).tolist()\n",
    "\n",
    "# Create the fuzz ratio on the list of tuples ceated\n",
    "all_datasets['ratio'] = [fuzz.token_sort_ratio(*i) for i in datasets_tuple]\n",
    "\n",
    "# Exclude those that have a low match ratio, the threshhold is set low because some matches have a low score\n",
    "all_datasets = all_datasets[all_datasets.ratio>fuzzy_ratio]\n",
    "\n",
    "# Drop all duplicates based on the defined columns and keep all the wanted ones\n",
    "final_df = all_datasets.drop_duplicates(subset=['Referencia','Municipio','Frequencias'])\n",
    "final_df = final_df[all_new_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0673e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate step to save the data\n",
    "final_df.to_csv('/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/popup_zipcodes.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b45ec9",
   "metadata": {},
   "source": [
    "### Join pop-ups + zipcodes with population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2da0acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fuzzy ratio used to include the data\n",
    "fuzzy_ratio = 80\n",
    "\n",
    "# Create new column to use to join both datasets\n",
    "population['merge']='all'\n",
    "final_df['merge']='all'\n",
    "\n",
    "# Join both datasets per row\n",
    "df_with_pop = pd.merge(final_df,population,on='merge')\n",
    "del df_with_pop['merge']\n",
    "\n",
    "# Create list of tuples based on the columns that we want to use for the join\n",
    "datasets_tuple_with_pop = df_with_pop[['Municipio', 'NOMBRE']].apply(tuple, axis=1).tolist()\n",
    "\n",
    "# Create the fuzz ratio on the list of tuples ceated\n",
    "df_with_pop['ratio'] = [fuzz.token_sort_ratio(*i) for i in datasets_tuple_with_pop]\n",
    "\n",
    "# Exclude those that have a low match ratio, the threshhold is set low because some matches have a low score\n",
    "df_with_pop = df_with_pop[df_with_pop.ratio>fuzzy_ratio]\n",
    "\n",
    "# Drop all duplicates based on the defined columns and keep all the wanted ones\n",
    "final_df_with_pop = df_with_pop.drop_duplicates(subset=['Referencia','Municipio','Frequencias'])\n",
    "\n",
    "final_df_with_pop = final_df_with_pop[['Referencia','Comunidad','referencia_Provincia','Municipio','Frequencias','codigopostalid','lat','lon','POB21']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bfcc226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate step to save the data\n",
    "final_df_with_pop.to_csv('/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/popup_zipcodes_with_population.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a518eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.head()\n",
    "# final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "12b667d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b2546",
   "metadata": {},
   "source": [
    "## Clean the main dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08466cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows that are unecessary\n",
    "# Function to return a list with unique numeric values\n",
    "def unique(list1):\n",
    "    x = np.array(list1)\n",
    "    return list(np.unique(x))\n",
    "\n",
    "# Creates list of the returned values \n",
    "list_cities = unique(main_df.Localidad)\n",
    "\n",
    "# Includes only text values, all cities of Spain\n",
    "new_list_cities = []\n",
    "for i in list_cities:\n",
    "    if i.isnumeric() is False:\n",
    "        if i != ' ':\n",
    "            new_list_cities.append(i)\n",
    "\n",
    "# Filter out all the values that are not inside the new_list_cities\n",
    "main_df = main_df.loc[main_df['Localidad'].isin(new_list_cities)]\n",
    "\n",
    "# --- Work on booleans\n",
    "\n",
    "# Fill in False to all nulls for specific boolean columns\n",
    "main_df[['Susceptible cesion','Susceptible mutualizacion','Obtenido por transferencia']] = \\\n",
    "                            main_df[['Susceptible cesion','Susceptible mutualizacion','Obtenido por transferencia']].fillna(False)\n",
    "                            \n",
    "main_df['Susceptible cesion'] = main_df['Susceptible cesion'].replace(\"true\", True)\n",
    "main_df['Obtenido por transferencia'] = main_df['Obtenido por transferencia'].replace(\"Detalle\", True)\n",
    "\n",
    "# --- Work on the dates\n",
    "\n",
    "# Select columns that contain dates\n",
    "date_columns = ['F. Caducidad','F. Concesion']\n",
    "\n",
    "# Transform date objects to datetime\n",
    "main_df[date_columns] = main_df[date_columns].apply(pd.to_datetime, errors='coerce',infer_datetime_format=True)\n",
    "\n",
    "# New features day, month and year\n",
    "main_df['total_dia_concesion'] = round((main_df['F. Caducidad'] - main_df['F. Concesion']).dt.days,0).fillna(0).apply(np.int64)\n",
    "main_df['total_mes_concesion'] = round((main_df['F. Caducidad'] - main_df['F. Concesion']).dt.days/12).fillna(0).apply(np.int64)\n",
    "main_df['total_año_concesion'] = round((main_df['F. Caducidad'] - main_df['F. Concesion']).dt.days/360).fillna(0).apply(np.int64)\n",
    "\n",
    "main_df['left_dia_concesion'] = round((main_df['F. Caducidad'] - dt.now()).dt.days,0).fillna(0).apply(np.int64)\n",
    "main_df['left_mes_concesion'] = round((main_df['F. Caducidad'] - dt.now()).dt.days/12).fillna(0).apply(np.int64)\n",
    "main_df['left_año_concesion'] = round((main_df['F. Caducidad'] - dt.now()).dt.days/360).fillna(0).apply(np.int64)\n",
    "\n",
    "# Rename column Provincia, after join with the Reference dataset it would become unclear what is what\n",
    "main_df = main_df.rename(columns={\"Provincia\": \"main_Provincia\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb71313",
   "metadata": {},
   "source": [
    "## Joined datasets\n",
    "Joining both datasets to create the final dataset that needs to be standarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "262c2329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join to only get the match that is on Referencia in both datasets\n",
    "df_joined = pd.merge(main_df,final_df_with_pop, how='inner', left_on = 'Referencia', right_on = 'Referencia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e5f40d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(870, 26)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_joined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb6006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate step to clean the data\n",
    "df_joined.to_csv('/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/df_joined.csv',sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
