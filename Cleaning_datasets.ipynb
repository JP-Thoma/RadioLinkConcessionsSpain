{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5975c92",
   "metadata": {},
   "source": [
    "# Cleaning the datasets for Registro Público de Concesiones and zipcodes\n",
    "\n",
    "This code includes all the cleaning done for the datasets that were scrapped and aggregates these into a big dataset.\n",
    "\n",
    "- [Zipcodes](https://xn--cdigospostales-lob.es/listado-de-codigos-postales-de-espana/)\n",
    "- [Main dataset](https://sedeaplicaciones.minetur.gob.es/RPC_Consulta)\n",
    "    - Main page dataset\n",
    "    - Pop-up datasets\n",
    "\n",
    "## Packages used\n",
    "\n",
    "- datetime\n",
    "- numpy\n",
    "- pandas\n",
    "- fuzzywuzzy\n",
    "- math\n",
    "- datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b7d4ed",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "a7b1305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from datetime import datetime as dt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99dacd",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "07242ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main dataset\n",
    "df = pd.read_csv('/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/new_data/fre.csv')\n",
    "# Pop-up dataset\n",
    "zipcodes = pd.read_excel('/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/new_data/listado-codigos-postales-con-LatyLon.xls')\n",
    "\n",
    "# Import all the datasets for all regions\n",
    "# region_1 = pd.read_csv('')\n",
    "# region_2 = pd.read_csv('')\n",
    "# region_3 = pd.read_csv('')\n",
    "# region_4 = pd.read_csv('')\n",
    "# region_5 = pd.read_csv('')\n",
    "# region_6 = pd.read_csv('')\n",
    "# region_7 = pd.read_csv('')\n",
    "# region_8 = pd.read_csv('')\n",
    "# region_9 = pd.read_csv('')\n",
    "# region_10 = pd.read_csv('')\n",
    "# region_11 = pd.read_csv('')\n",
    "# region_12 = pd.read_csv('')\n",
    "# region_13 = pd.read_csv('')\n",
    "# region_14 = pd.read_csv('')\n",
    "# region_15 = pd.read_csv('')\n",
    "# region_16 = pd.read_csv('')\n",
    "# region_17 = pd.read_csv('')\n",
    "# region_18 = pd.read_csv('')\n",
    "# region_19 = pd.read_csv('')\n",
    "\n",
    "# Concat all the datasets\n",
    "# df_concat = pd.concat([\n",
    "#                         region_1,\n",
    "#                         region_2,\n",
    "#                         region_3,\n",
    "#                         region_4,\n",
    "#                         region_5,\n",
    "#                         region_6,\n",
    "#                         region_7,\n",
    "#                         region_8,\n",
    "#                         region_9,\n",
    "#                         region_10,\n",
    "#                         region_11,\n",
    "#                         region_12,\n",
    "#                         region_13,\n",
    "#                         region_14,\n",
    "#                         region_15,\n",
    "#                         region_16,\n",
    "#                         region_17,\n",
    "#                         region_18,\n",
    "#                         region_19,\n",
    "#                         ])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e38120",
   "metadata": {},
   "source": [
    "# Clean the pop-up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "c7299195",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/Cleaning_datasets.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/Cleaning_datasets.ipynb#ch0000002?line=10'>11</a>\u001b[0m fuzzy_ratio \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/Cleaning_datasets.ipynb#ch0000002?line=12'>13</a>\u001b[0m \u001b[39m# Clean all the pop-up datasets\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/Cleaning_datasets.ipynb#ch0000002?line=13'>14</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: x\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39;49mstrip())\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, np\u001b[39m.\u001b[39mnan)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/Cleaning_datasets.ipynb#ch0000002?line=14'>15</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mfillna(method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mffill\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/Cleaning_datasets.ipynb#ch0000002?line=15'>16</a>\u001b[0m df[[\u001b[39m'\u001b[39m\u001b[39mFrequencias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTipo\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mFrecuencia\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m, expand\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:8839\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8828\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   8830\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   8831\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   8832\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8837\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   8838\u001b[0m )\n\u001b[0;32m-> 8839\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py:727\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    725\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 727\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py:851\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    853\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py:867\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    865\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    866\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    868\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    869\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    870\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    871\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/Cleaning_datasets.ipynb Cell 7\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/Cleaning_datasets.ipynb#ch0000002?line=10'>11</a>\u001b[0m fuzzy_ratio \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/Cleaning_datasets.ipynb#ch0000002?line=12'>13</a>\u001b[0m \u001b[39m# Clean all the pop-up datasets\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/Cleaning_datasets.ipynb#ch0000002?line=13'>14</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39mstrip())\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, np\u001b[39m.\u001b[39mnan)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/Cleaning_datasets.ipynb#ch0000002?line=14'>15</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mfillna(method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mffill\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/Cleaning_datasets.ipynb#ch0000002?line=15'>16</a>\u001b[0m df[[\u001b[39m'\u001b[39m\u001b[39mFrequencias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTipo\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mFrecuencia\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m, expand\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5568\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5569\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   5570\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   5571\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   5572\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5573\u001b[0m ):\n\u001b[1;32m   5574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> 5575\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/accessor.py:182\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[39m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessor\n\u001b[0;32m--> 182\u001b[0m accessor_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor(obj)\n\u001b[1;32m    183\u001b[0m \u001b[39m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m# NDFrame\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/strings/accessor.py:177\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m    175\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstring_\u001b[39;00m \u001b[39mimport\u001b[39;00m StringDtype\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate(data)\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_categorical \u001b[39m=\u001b[39m is_categorical_dtype(data\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_string \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(data\u001b[39m.\u001b[39mdtype, StringDtype)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/strings/accessor.py:231\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    228\u001b[0m inferred_dtype \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39minfer_dtype(values, skipna\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m inferred_dtype \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_types:\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan only use .str accessor with string values!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    Doing a fuzzy join on the dataset of zipcodes and the data of reference\n",
    "    data inside the 'Consulta del Registro Público de Concesiones'.\n",
    "    This is needed, because the data from the source does not have zipcodes, longitute and latitude.\n",
    "    The fuzzy join does a cross join, then calculates the fuzzy ratio and include the data based on a fuzzy_ratio.\n",
    "    Then, the duplicated columns are dropped.\n",
    "    The output is a new dataset that includes the zipcode, longitude and latitude per tower.\n",
    " \"\"\"\n",
    "\n",
    "# Define the fuzzy ratio used to include the data\n",
    "fuzzy_ratio = 20\n",
    "\n",
    "# Clean all the pop-up datasets\n",
    "df = df.drop(df.columns[[0]],axis = 1)\n",
    "df = df.apply(lambda x: x.str.strip()).replace('', np.nan)\n",
    "df = df.fillna(method='ffill')\n",
    "df[['Frequencias', 'Tipo']] = df['Frecuencia'].str.split(' ', 1, expand=True)\n",
    "del df['Frecuencia']\n",
    "df['Frequencias'] = df['Frequencias'].apply(lambda x: x.replace('.', '')).apply(lambda x: x.replace(',', '.')).astype('float')\n",
    "\n",
    "# --- \n",
    "\n",
    "# Cleaning of zipcodes\n",
    "\n",
    "# Define the columns that we want to include in the final dataset\n",
    "zipcodes_columns = ['codigopostalid','lat','lon']\n",
    "df_columns = [ 'Referencia','Comunidad','Provincia','Municipio','Frequencias','Tipo']\n",
    "all_new_columns = df_columns+zipcodes_columns\n",
    "\n",
    "# Create new column to use to join both datasets\n",
    "zipcodes['merge']='all'\n",
    "df2['merge']='all'\n",
    "\n",
    "# Join both datasets per row\n",
    "all_datasets = pd.merge(df2,zipcodes,on='merge')\n",
    "del all_datasets['merge']\n",
    "\n",
    "# Create list of tuples based on the columns that we want to use for the join\n",
    "datasets_tuple = all_datasets[['Municipio', 'poblacion']].apply(tuple, axis=1).tolist()\n",
    "\n",
    "# Create the fuzz ratio on the list of tuples ceated\n",
    "all_datasets['ratio'] = [fuzz.token_sort_ratio(*i) for i in datasets_tuple]\n",
    "\n",
    "# Exclude those that have a low match ratio, the threshhold is set low because some matches have a low score\n",
    "all_datasets = all_datasets[all_datasets.ratio>fuzzy_ratio]\n",
    "\n",
    "# Drop all duplicates based on the defined columns and keep all the wanted ones\n",
    "final_df = all_datasets[all_new_columns].drop_duplicates(subset=['Referencia','Municipio','Frequencias'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b2546",
   "metadata": {},
   "source": [
    "# Clean the main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "18de83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('/Users/niko/Documents/Personal/GitHub/RadioLinkConcessionsSpain/RegistroPublicoConcesiones_General.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "08466cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Remove the rows that are unecessary\n",
    "\n",
    "# Function to return a list with unique numeric values\n",
    "def unique(list1):\n",
    "    x = np.array(list1)\n",
    "    return list(np.unique(x))\n",
    "\n",
    "# Creates list of the returned values \n",
    "list_cities = unique(main_df.Localidad)\n",
    "\n",
    "# Includes only text values, all cities of Spain\n",
    "new_list_cities = []\n",
    "for i in list_cities:\n",
    "    if i.isnumeric() is False:\n",
    "        if i != ' ':\n",
    "            new_list_cities.append(i)\n",
    "\n",
    "# Filter out all the values that are not inside the new_list_cities\n",
    "main_df = main_df.loc[main_df['Localidad'].isin(new_list_cities)]\n",
    "\n",
    "# --- Work on booleans\n",
    "\n",
    "# Fill in False to all nulls for specific boolean columns\n",
    "main_df[['Susceptible cesion','Susceptible mutualizacion','Obtenido por transferencia']] = \\\n",
    "                            main_df[['Susceptible cesion','Susceptible mutualizacion','Obtenido por transferencia']].fillna(False)\n",
    "                            \n",
    "main_df['Susceptible cesion'] = main_df['Susceptible cesion'].replace(\"true\", True)\n",
    "main_df['Obtenido por transferencia'] = main_df['Obtenido por transferencia'].replace(\"Detalle\", True)\n",
    "\n",
    "# --- Work on the dates\n",
    "\n",
    "# Select columns that contain dates\n",
    "date_columns = ['F. Caducidad','F. Concesion']\n",
    "\n",
    "# Transform date objects to datetime\n",
    "main_df[date_columns] = main_df[date_columns].apply(pd.to_datetime, errors='coerce')\n",
    "\n",
    "# New features day, month and year\n",
    "main_df['dia_concesion'] = round((main_df['F. Caducidad'] - main_df['F. Concesion']).dt.days,0).fillna(0).apply(np.int64)\n",
    "main_df['mes_concesion'] = round((main_df['F. Caducidad'] - main_df['F. Concesion']).dt.days/12).fillna(0).apply(np.int64)\n",
    "main_df['año_concesion'] = round((main_df['F. Caducidad'] - main_df['F. Concesion']).dt.days/360).fillna(0).apply(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb71313",
   "metadata": {},
   "source": [
    "# Joined datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "262c2329",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = pd.merge(main_df,final_df, how='left', left_on = 'Referencia', right_on = 'Referencia')\n",
    "\n",
    "df_joined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
